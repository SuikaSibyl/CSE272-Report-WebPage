(function(){"use strict";var e={933:function(e,t,i){var a=i(963),s=i(174),n=i(828),l=(i(415),i(252));function o(e,t,i,a,s,n){const o=(0,l.up)("ReportPage");return(0,l.wg)(),(0,l.j4)(o)}const r=e=>((0,l.dD)("data-v-6137ba46"),e=e(),(0,l.Cn)(),e),u={class:"home"},m={class:"common-layout"},c=r((()=>(0,l._)("h1",{id:"Report For CSE 274"},[(0,l.Uk)("Implementation of BDPT and MMLT on GPU with"),(0,l._)("br"),(0,l.Uk)(" Simple Primal Sample Space Latent Nueral Mutation")],-1))),p=r((()=>(0,l._)("br",null,null,-1))),h=r((()=>(0,l._)("h2",{id:"1. Introduction",style:{"text-align":"left"}},"1. Introduction",-1))),d=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l.Uk)(" In this project, I implemented BDPT (Bidirectional Path Tracing) "),(0,l._)("a",{href:"#cite-veach"},"[1]"),(0,l.Uk)(" and MMLT (Multiplexed metropolis Light Transport) "),(0,l._)("a",{href:"#cite-mmlt"},"[2]"),(0,l.Uk)(" on my GPU renderer, "),(0,l._)("a",{href:"https://github.com/SuikaSibyl/SIByLEngine2023"},"SIByL Engine"),(0,l.Uk)(". Also I have some naive ideas and experiments on Neural Mutation for Metropolis light transport as well as common Metropolis-Hastings sampling. ")],-1))),f=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," The render part is implemented in C++ with Vulkan, and the implementation is only compatible with Windows OS and Nvidia RTX GPU. All the results are run and measured on my personal RTX 3070 laptop. The neural part is implemented in Python with PyTorch. The inter-process communication is done with localhost socket. ",-1))),g=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l.Uk)(" In "),(0,l._)("a",{href:"#2."},"Section 2"),(0,l.Uk)(" I will breifly talk about how I implement BDPT and MMLT on GPU, without lots of details, and show some results and comparison with unidirectional path tracing. ")],-1))),w=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l.Uk)(" Then, "),(0,l._)("a",{href:"#3."},"Section 3"),(0,l.Uk)(' discussed something about normalizing flow and neural importance sampling. I will also show some results and comparison in 2D monochrome image case. They are talked about because they are the basis of my "Latent Space Mutation" idea. ')],-1))),y=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l.Uk)(" And "),(0,l._)("a",{href:"#4."},"Section 4"),(0,l.Uk)(' talks about "Latent Space Mutation", a (probably) new concept I proposed, and why I think it might be useful. Some experiments are also shown both in 2D monochrome image case and single-depth MMLT case. ')],-1))),b=r((()=>(0,l._)("h2",{id:"2."},"2. BDPT & MMLT on GPU",-1))),_=r((()=>(0,l._)("h3",{id:"2.1",style:{"margin-top":"0in"}},"2.1 Brief Introduction",-1))),k=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l.Uk)(" BDPT and MLT are proposed by Veach "),(0,l._)("a",{href:"#cite-veach"},"[1]"),(0,l.Uk)(". PSSMLT "),(0,l._)("a",{href:"#cite-pssmlt"},"[4]"),(0,l.Uk)(" is a simple version of the vanilla MLT that do mutation in primal sample space. And MMLT "),(0,l._)("a",{href:"#cite-mmlt"},"[2]"),(0,l.Uk)(" is a different way of doing PSSMLT, that has a static path depth for each Markov Chain. I implement both BDPT and MLT in my renderer referring to the implementation of PBRT "),(0,l._)("a",{href:"#cite-pbrt"},"[3]"),(0,l.Uk)(". Volumetric rendering is not supported in this project. ")],-1))),x=r((()=>(0,l._)("h3",{id:"2.1",style:{"margin-top":"0in"}},"2.2 Implementation and Experiments",-1))),v=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," For both BDPT and MLT, splatting is required for contributing to the film. As far as I could tell, there are three ways to do splatting. (1) we could use atomicAdd to add the contribution to the film, (2) we could use a atomic buffer to do per-pixel spinlock and do mutex adding, (3) we could use a temporary buffer to store the contribution, and then dispatch a set of pixel-size fragment shader to do adding alpah blending. I use the first method, although it need three atomic buffer instead of one, the performance is probabily better than the second method, I actually could not compare it because my per-pixel spinlock get deadlock on raygen shader warps although it works fine for compute shader... ",-1))),W=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," To implement BDPT, I use only one large kernel, tracing 1 spp BDPT for each pixel. I did not optimize the kernel too much, but reduce the original cost of 170+ ms per frame to 10+ms per frame by a simple trick: manually unrolling the loop for path connection. The main reason is not carefully tested, but I guess it is because unrolling prevent some fake loop carried dependency, which is harmful for pipeline. I just observed severe long scoreboard stall when I use the original loop, and the stall is gone after unrolling. ",-1))),T=r((()=>(0,l._)("p",{style:{"text-align":"left"}},' To implement MMLT, I designed a dynamic pass pipeline for interactive frame rate. The first part should be boostrap sample generation. We should generate more boostrap samples to choosen from and compute average b, but running all samples in one frame is too slow. Therefore we could armotize the cost by running a few samples in each frame. After some frames, we then begin mutation pass. For each frame we run one mutation for one Markov Chain per thread. Notice that the first frame in "mutation" pass is not mutation, but choose samples from boostrap. I use hierarchical 2d mip for sampling boostrap samples. The per-frame task assignment is shown below: ',-1))),U=r((()=>(0,l._)("p",null,[(0,l._)("code",null," My cross-frame MMLT pipeline. ")],-1))),I=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," An interesting design I want to mention is how to get PSS sample from boostrap. We have far more boostrap samples than chains per frame, if we store the PSS samples for boostrap samples, it would need a temporary buffer which is large and would not be used later. Instead, I recover the random seed for the boostrap sample chosen, and generate the PSS sample sequence on the fly. ",-1))),j=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," BDPT has better convergence rate than UDPT in many cases. In the demo scene shown below, BDPT is also slightly better than UDPT. The scene is just a simple set of still lifes in the room from Veach's MLT scene. The only light is a sphere in the next room, and the camera is in the room with the still lifes. The scene is rendered with 500 spp, and the result is shown below. ",-1))),M=r((()=>(0,l._)("div",{id:"udptvsbdpt"},null,-1))),P=r((()=>(0,l._)("p",null,[(0,l._)("code",null,"left: unidirectional 500 spp | right: bidirectional 500 spp")],-1))),L=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," MMLT has better results than BDPT, as shown in the figure below. (Even though lookes like to have more fireflies somehow.) We could clearly see that MLT is less noisy than BDPT under same spp. But as I am using MMLT, it actually take much more time because it need at least 4 times path numbers as MMLT trace one path for each depth, so for a 3-bounce path, it need 3 more paths per sample. And I realize that this dummy MMLT is less efficient thant BDPT, as BDPT actually has some kind of coherency in neighbor paths, this dummy MMLT is completely random and has larger divergence. ",-1))),N=r((()=>(0,l._)("div",{id:"bdptvsmlt"},null,-1))),S=r((()=>(0,l._)("p",null,[(0,l._)("code",null,"left: bidirectional 500 spp | right: mmlt 500 mpp")],-1))),A=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," Actually I should do some obvious optimizing for MMLT on GPU. As for each Markov Chain, the depth is fixed, so we could actually do some sort after choosing boostrap path, and do 4 different tracing drawcall to issue 4 bundles of chains with different depth. This could reduce lots of unnecessary thread divergence, and easily achieve coherent with only one sort (actually only one radix sort pass or some compaction passes) per rendering, which is much simpler than optimizing BDPT for GPU. ",-1))),B=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," In my observation, thread divergence is the main problem for BDPT and MLT, so I beleive this optimization is really worth to implement and could hopefully boost the performance. But as the deadline is approaching, I have to leave this optimization for future work. ",-1))),C=r((()=>(0,l._)("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/tmI0e2OMLN4",title:"YouTube video player",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowfullscreen:""},null,-1))),D=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l._)("br"),(0,l.Uk)("Both algorithms are integrated into my interactive renderer, as shown in the video. I guess the one of the most important target for porting these algorithm into an interactive renderer is to limit the cost per frame. Otherwise a long stall would get the whole system stuck. In cpu applications we could use multi-threading to decouple UI and rendering, but as I am using ImGui, getting the GPU queue stack would also affect the UI. And of course it would be important to optimize the code for GPU architecture for better performance. ")],-1))),Z=r((()=>(0,l._)("h2",{id:"3."},"3. Neural Importance Sampling",-1))),z=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l.Uk)(" The object of the project is about exploring the neural mutation for MLT, so it might seems a little bit off-topic to talk about neural importance sampling here. But as normalizing flow is a very useful technique for generative models, I think it is worth to talk about it a little bit. Neural importance sampling is probably the first work that I know that uses normalizing flow in rendering, and I personally think it could be used in neural mutation, see "),(0,l._)("a",{href:"#4."},"Section 4"),(0,l.Uk)(" for more details. ")],-1))),O=r((()=>(0,l._)("h3",{id:"3.1"},"3.1 Theory behind NIS",-1))),R=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," Normalizing flow is an useful technique to generate samples subject to some certain distributions by a neural network. But the really interesting part that makes it different from other generative models is that it actually creates a bijection between two distributions, which means the jacobian of the transformation is computable. ",-1))),F=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l.Uk)(" NICE "),(0,l._)("a",{href:"#cite-nice"},"[5]"),(0,l.Uk)(' proposed a simple way to construct a bijection by using a structure called "Coupling Layer". RealNVP '),(0,l._)("a",{href:"#cite-realNVP"},"[6]"),(0,l.Uk)(' further extend that into "Affine Coupling Layer", which introduces non-volume-preserving transformation in each layer and is thus more powerful. GLOW '),(0,l._)("a",{href:"#cite-glow"},"[7]"),(0,l.Uk)(" proposed another primitive to construct a bjicetion, the invertible 1x1 Convolutions. ")],-1))),X=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l.Uk)(" Müller et al. "),(0,l._)("a",{href:"#cite-nis"},"[8]"),(0,l.Uk)(" and Zheng et al. "),(0,l._)("a",{href:"#cite-realNVP"},"[9]"),(0,l.Uk)(" introduce normalizing flow into rendering and use it for neural importance sampling. Generally speaking, both of them use a normalizing flow to build a bijection between [0,1] uniform distribution and the target radiance distribution, so that they could do importance sampling. ")],-1))),V={style:{"text-align":"left"}},q=r((()=>(0,l._)("a",{href:"#cite-realNVP"},"[9]",-1))),G=r((()=>(0,l._)("p",null,[(0,l._)("code",null,[(0,l.Uk)(" Computational structure of Zheng et al.'s neural importance sampling model based on Real NVP "),(0,l._)("br"),(0,l.Uk)(" from "),(0,l._)("a",{href:"#cite-realNVP"},"[9]"),(0,l.Uk)(" . I implement almost the same thing in PyTorch for later experiments. ")])],-1))),E={style:{"text-align":"left"}},H=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l.Uk)(" However, in training, they use the traditional loss function for normalizing flow, a.k.a. maximizing the log likelihood of generating target distribution. This implies we need to sample from the target distribution, which is not straightforward in rendering. But we could still do it by some indirect way, like MCMC or resampling "),(0,l._)("a",{href:"#cite-ris"},"[10]"),(0,l.Uk)(". ")],-1))),K=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l.Uk)(" Müller et al. "),(0,l._)("a",{href:"#cite-nis"},"[8]"),(0,l.Uk)(" gives some advanced contributions including a more powerful piecewise-polynomial coupling layer, and deriving two new loss functions for neural importance sampling. Interestingly, they showed that minimizing the KL divergence amounts to maximizing a weighted log likelihood. ")],-1))),$={style:{"text-align":"left"}},J={style:{"text-align":"left"}},Y=r((()=>(0,l._)("del",null,"although I guess using differentiable rendering for PSS is probabaly helpful?",-1))),Q={style:{"text-align":"left"}},ee=r((()=>(0,l._)("h3",{id:"3.2"},"3.2 Implementation and Experiments",-1))),te=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l.Uk)(" I implemented NICE "),(0,l._)("a",{href:"#cite-nice"},"[5]"),(0,l.Uk)(" and RealNVP "),(0,l._)("a",{href:"#cite-realNVP"},"[6]"),(0,l.Uk)(" in PyTorch framework. The specific implementation details for t/s transform are not declared in the papers, for NICE I use MLP for t transform, and for RealNVP I use a 2-MLP residual blocks for t-s transform (as Zheng et al. "),(0,l._)("a",{href:"#cite-realNVP"},"[9]"),(0,l.Uk)(" did). As RealNVP is more powerful than NICE, all the following experiments are based on RealNVP. ")],-1))),ie=r((()=>(0,l._)("p",{style:{"text-align":"left"}},' First, I checked how the number of layers influence the expressivity. The "converged" results are shown in the following figure. But notice that all the results are not fully converged, I just cut the training process after their convergence becomes super slow. ',-1))),ae=r((()=>(0,l._)("p",null,[(0,l._)("code",null,[(0,l.Uk)(" (0): the target image "),(0,l._)("a",{href:"https://pixabay.com/photos/statue-sculpture-figure-1275469/"},"stone sculpture"),(0,l.Uk)(" / distribution (rings) "),(0,l._)("br"),(0,l.Uk)(" (1): the same targets, but here darker pixels refers to higher density. "),(0,l._)("br"),(0,l.Uk)(" (2): (quasi) convergenced distribution by 4 coupling layer RealNVP. "),(0,l._)("br"),(0,l.Uk)(" (3): (quasi) convergenced distribution by 8 coupling layer RealNVP. ")])],-1))),se=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l.Uk)(" We could clearly find that more numbers of coupling layers could have stronger expresivity. (a/b-3) fits the objective distribution better thant (a/b-2). This is exactly what we expected. It is said that Piecewise-polynomial coupling layer by Müller et al. "),(0,l._)("a",{href:"#cite-nis"},"[8]"),(0,l.Uk)(" has better expressivity for each layer, and thus could reduce the total number of layers, but I did not implement it here. ")],-1))),ne=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," Before training towards any complex distribution, we first start from a checkpoint that fits uniform bijection well. The process of training towards a uniform distribution is shown below: ",-1))),le=r((()=>(0,l._)("p",null,[(0,l._)("code",null," How RealNVP progressively learned to be a uniform bijection. ")],-1))),oe=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l.Uk)(" Then we train towards the target distribution. There are two ways to do it: "),(0,l._)("br"),(0,l.Uk)(" (1) "),(0,l._)("b",null,"Inverse Training"),(0,l.Uk)(": Traditional way to do it in normalizing flow. Given X samples subject to target distribution, use Inverse transform to find corresponding latent vairable Z, and maximize the likelihood of generating X. Here we generate X samples by Metropolis-Hastings sampling with average 20 mutations per pixel. "),(0,l._)("br"),(0,l.Uk)(" (2) "),(0,l._)("b",null,"Forward Training:"),(0,l.Uk)(": According to Müller et al. "),(0,l._)("a",{href:"#cite-nis"},"[8]"),(0,l.Uk)(" it is also possible to start from Z samples subject to priori distribution, use forward transform to find corresponding X samples, and maximize the KL divergence between F(X) values and pdf of X. In this process we do not need to generate samples subject to target distribution."),(0,l._)("br")],-1))),re=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l.Uk)(" I tried both and the results are shown below, both of them are using 16 affine coupling layers. I am "),(0,l._)("b",null,"NOT"),(0,l.Uk)(" sure that there is no bug in forward training. But it seems that even though forward training could capture the main structure of the target distribution, it could hardly know where to go and get stuck at some low frequency status. This is not expected, because in Müller et al. "),(0,l._)("a",{href:"#cite-nis"},"[8]"),(0,l.Uk)(" get fairly not-that-bad results even when using affine layers. For further investigation, I will try to implement Piecewise-polynomial coupling layer and check the correctness of forward training code. But for the following experiments, I will only use inverse training for quality and simplicity. ")],-1))),ue=r((()=>(0,l._)("p",null,[(0,l._)("code",null," How RealNVP progressively learned to be a uniform bijection. ")],-1))),me=r((()=>(0,l._)("h2",{id:"4."},"4. Latent Space Mutation",-1))),ce=r((()=>(0,l._)("h3",{id:"4.1",style:{"margin-top":"0in"}},"4.1 What Is & Why Latent Space Mutation",-1))),pe=r((()=>(0,l._)("h4",{style:{"margin-top":"0.1in"}},[(0,l.Uk)("What is Latent Space Mutation"),(0,l._)("br")],-1))),he=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," I proposed this concept to hopefully improve mutation for MLT, which is some kind of combination of importance sampling and Metropolis-Hastings sampling. I think the two closely related concepts are neural importance sampling and hidden markov model. ",-1))),de=r((()=>(0,l._)("p",null,[(0,l._)("code",null," Vanilla Metropolis-Hastings. ")],-1))),fe=r((()=>(0,l._)("p",null,[(0,l._)("code",null," Hidden Markov Model. ")],-1))),ge={style:{"text-align":"left"}},we=r((()=>(0,l._)("p",null,[(0,l._)("code",null," Vanilla Metropolis-Hastings. ")],-1))),ye={style:{"text-align":"left"}},be=r((()=>(0,l._)("br",null,null,-1))),_e=r((()=>(0,l._)("br",null,null,-1))),ke=r((()=>(0,l._)("br",null,null,-1))),xe=r((()=>(0,l._)("br",null,null,-1))),ve=r((()=>(0,l._)("br",null,null,-1))),We=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l.Uk)(' Therefore we essentially want to do mutation in latent space instead of primal space. Here, the "common strategy" in step (3) refers to simple strategies like a uniform sampling large step and a normal sampling small step, mentioned in PBRT '),(0,l._)("a",{href:"#cite-pbrt"},"[3]"),(0,l.Uk)(". ")],-1))),Te=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," But why should we do this? We could see how large step and small step works in latent space to get intuition. ",-1))),Ue=r((()=>(0,l._)("h4",{style:{"margin-top":"0.in"}},"Ideal Large Step Mutation",-1))),Ie={style:{"text-align":"left"}},je={style:{"text-align":"left"}},Me={style:{"text-align":"left"}},Pe=r((()=>(0,l._)("h4",{style:{"margin-top":"0.in"}},"Ideal Small Step Mutation",-1))),Le=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," In small step case, things become a little bit subtle. Ideally, the mapping from Z to X is some kind of warping that makes uniform Z concentrated to the dense regions of X. In a ideal case the mapping is super smooth and regular (in practice, RealNVP could hardly achieve this). ",-1))),Ne=r((()=>(0,l._)("p",null,[(0,l._)("code",null," Mapping from Z to X is some warping. ")],-1))),Se=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," But let' assume this happens. We do a isotropic Gaussian from the start state (red point) and produce two possible mutated positions (blue and green points). If we do this mutation is primal step, they would located in low-probability regions and are likely to be rejected. On the other hand, if we do this mutation in latent space, This samples would be warpped by the mapping and agian located in high-probability regions. The rotation and scaling of the Gaussian are just adaptive, a narrow axis would be squeezed by the warping and result in a more narrow Gaussian, as shown in the figure below. ",-1))),Ae=r((()=>(0,l._)("p",null,[(0,l._)("code",null,[(0,l.Uk)(" The first row: latent space mutation. "),(0,l._)("br"),(0,l.Uk)(" The second row: primal space mutation. ")])],-1))),Be={style:{"text-align":"left"}},Ce=r((()=>(0,l._)("h4",{style:{"margin-top":"0.in"}},"To be NOT ideal",-1))),De=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," Interestingly, we could see that in ideal case, both large step and small step mutations are just importance sampling. The large step is just how we commonly do importance sampling, while the small step is a random walk version of it. The isotropic random walk in latent space finally would create a sequence of uniform samples in latent space and would be mapped to exactly the target distribution in primal space. ",-1))),Ze=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," The truth is, things are NOT ideal. But this truth would not frustrate us, we could see that sub-optimal importance sampling still helps us a lot in convergence rate. Thus we could expect that a suboptimal neural (and even non-neural) latent space mutation would also help us, as some kind of interpolation between optimal importance sampling and dummy metropolis-hastings. ",-1))),ze={style:{"text-align":"left"}},Oe=r((()=>(0,l._)("h3",{id:"4.2",style:{"margin-top":"0in"}},"4.2 Monochrome 2D Distribution",-1))),Re=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," To quickly verify the design, we do experiments on a simple monochrome 2D distribution. ",-1))),Fe=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," The distribution tested is an anisotropy oval, the target distribution and the learned results are: ",-1))),Xe=r((()=>(0,l._)("p",null,[(0,l._)("code",null,[(0,l.Uk)(" Left: the target distribution "),(0,l._)("br"),(0,l.Uk)(" Right : the learned distribution. ")])],-1))),Ve=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," The results are also very good. We could observe both reconstruction results and acceptance rates are relatively better when we adopt the neural latent space sampling: ",-1))),qe=r((()=>(0,l._)("p",{style:{"text-align":"left"}},null,-1))),Ge=r((()=>(0,l._)("h3",{id:"4.2",style:{"margin-top":"0in"}},"4.3 Neural Mutate the Metropolis Light Transport",-1))),Ee=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," I also tried to use the neural mutation to mutate the Metropolis Light Transport. The result is not very good, but I think it is still worth to share. ",-1))),He=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," It is actually quite a disaster when implementating this: NaNs are everywhere. As soon as I raise the dimension from 2D to 48-dim, the gradient becomes unstable, if the layers are too deep, NaNs will quickly appear in gradients and then in loss, as far as I checked it is kind of related to logit-sigmoid mapping which do exponential operations. As a result I only use 4 coupling layers with 48 dimensions (I use at most 48 PSS random variables for a path). ",-1))),Ke=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," For MMLT, we have different depth so I think we should use different models for different depths. We could consider a path that is important for depth 4, then its subpath would not be important for depth 3 unless the last vertex is also on light source. In this experiment, I only test the case that has depth 4. ",-1))),$e=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," The test scene is a glass Stanford bunny, we could see that most important path with depth 4 are on the bunny: ",-1))),Je=r((()=>(0,l._)("div",{id:"alldepthvsdpeth4"},null,-1))),Ye=r((()=>(0,l._)("p",null,[(0,l._)("code",null,"left: result with depth 0-4 | right: only using depth 4")],-1))),Qe=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," Bothered by the NaNs, I only use 4 layers so the expressivity is significantly not that good in high dimensions. The figure below shows the comparison between the target distribution and the learned distribution. The target distribution are generated by the MLT, and all the images are showing their projection on 2 of 48 dims. ",-1))),et=r((()=>(0,l._)("p",null,[(0,l._)("code",null," Comparison between target and learned distribution. Notice that the target distribution are curved to show subtle details. ")],-1))),tt=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," And in actual rendering we need to input current sample stream to the neural network every frame, so we need to do inter-process communication. I choose to do this by using a socket on localhost, which is actually not that slow. The whole frame including MMLT tracing, neural mutation and socket communication runs at 3~4 fps. I believe with some optimization we could reach some real-time frame rate. The whole pipeline for each frame is shown below: ",-1))),it=r((()=>(0,l._)("p",null,[(0,l._)("code",null," My pipeline per frame. ")],-1))),at=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," First we test the case that we always use large step mutation. This generally works as expected that using latent neural mutation could concentrate more on important regions just as importance sampling does. The figure shows how they look like after 50 frames: ",-1))),st=r((()=>(0,l._)("div",{id:"largestep"},null,-1))),nt=r((()=>(0,l._)("p",null,[(0,l._)("code",null,"left: Using primal mutation | right: Using latent mutation ")],-1))),lt=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," Then we test the case that we always use small step mutation. And this turns out to be a failure. I beleive this is the part that is really valuable so I think I should really spend more time figuring out why. The figure shows how they look like after 200 frames: ",-1))),ot=r((()=>(0,l._)("div",{id:"smallstep"},null,-1))),rt=r((()=>(0,l._)("p",null,[(0,l._)("code",null,"left: Using primal mutation | right: Using latent mutation ")],-1))),ut=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," It seems that the latent mutation is more noisy but less correlated. But it is not better in acceptance rate, opposite to what I expected. Using primal space mutation has acceptance rate of 0.62, while using latent space mutation has acceptance rate of 0.35. It is really frustrating. ",-1))),mt=r((()=>(0,l._)("p",null,[(0,l._)("code",null," Non-optimal convergence to target distribution. ")],-1))),ct=r((()=>(0,l._)("p",{style:{"text-align":"left"}},' I think a possible reason is by RealNVP converges to something "not regular". Here "optimal transport" might be a more accurate word. We could see in practice that the mapping does not go in the optimal transport way, but result in somme strange distortion. They would have quite similar distribution with the target distribution, and this is not a problem for importance sampling. ',-1))),pt=r((()=>(0,l._)("p",null,[(0,l._)("code",null," Problematic mapping. ")],-1))),ht=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," But in latent space mutation, it could be harmful. We could observe that two points very close in primal space could be mapped into two far away points in latent space. The distance / neighbor properties are not preserved well. This will cause latent mutation could not traverse the neighbor region well and making things noisy. ",-1))),dt=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l.Uk)(" To solve this, I should try to constraint the neural network to do optimal transport. There are already some works on this, like OT-Flow "),(0,l._)("a",{href:"#cite-otflow"},"[11]"),(0,l.Uk)(" and CP-Flow "),(0,l._)("a",{href:"#cite-cpflow"},"[12]"),(0,l.Uk)(". But I did not have time to read them yet. Let's leave this as future work. ")],-1))),ft=r((()=>(0,l._)("h2",{id:"5."},"5. Future Works",-1))),gt=r((()=>(0,l._)("p",{style:{"text-align":"left"}}," Clearly the project is not completed yet. Here are some possible future works to do: ",-1))),wt=r((()=>(0,l._)("p",{style:{"text-align":"left"}},[(0,l.Uk)(" (1). Check the transition probability whether need Jacobian. (I guess the answer is yes)"),(0,l._)("br"),(0,l.Uk)(" (2). Try more on the forward training and piecewise-polynomial coupling layer. "),(0,l._)("br"),(0,l.Uk)(" (3). Try non-neural importance mapping (like cosine-weighted or Gaussian like mapping ?). "),(0,l._)("br"),(0,l.Uk)(" (4). Try training a more powerful and efficient high-dim model. "),(0,l._)("br"),(0,l.Uk)(" (5). Learn more about the optimal-transport stuff. "),(0,l._)("br"),(0,l.Uk)(" (6). Optimize BDPT and MLT for GPU. "),(0,l._)("br"),(0,l.Uk)(" (7). Try integrating neural mutation into C++ pipeline by CUDA / Vulkan ML like things. "),(0,l._)("br"),(0,l.Uk)(" (8). ...... ")],-1))),yt=r((()=>(0,l._)("h2",{id:"Reference",style:{"text-align":"left"}},"Reference",-1))),bt=r((()=>(0,l._)("ol",{style:{"padding-left":"20px"}},[(0,l._)("li",{id:"cite-veach",style:{"text-align":"left","list-style-type":"decimal","list-style-position":"outside"}},[(0,l._)("a",{href:"http://graphics.stanford.edu/papers/veach_thesis/"}," Eric Veach. 1998. Robust monte carlo methods for light transport simulation. Ph.D. Dissertation. Stanford University, Stanford, CA, USA. Advisor(s) Leonidas J. Guibas. Order Number: AAI9837162. ")]),(0,l._)("li",{id:"cite-mmlt",style:{"text-align":"left","list-style-type":"decimal","list-style-position":"outside"}},[(0,l._)("a",{href:"https://dl.acm.org/doi/10.1145/2601097.2601138"}," Toshiya Hachisuka, Anton S. Kaplanyan, and Carsten Dachsbacher. 2014. Multiplexed metropolis light transport. ACM Trans. Graph. 33, 4, Article 100 (July 2014), 10 pages. https://doi.org/10.1145/2601097.2601138 ")]),(0,l._)("li",{id:"cite-pbrt",style:{"text-align":"left","list-style-type":"decimal","list-style-position":"outside"}},[(0,l._)("a",{href:"https://rgl.epfl.ch/software/PBRT"}," Matt Pharr, Wenzel Jakob, and Greg Humphreys. 2016. Physically Based Rendering: From Theory to Implementation (3rd ed.). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA. ")]),(0,l._)("li",{id:"cite-pssmlt",style:{"text-align":"left","list-style-type":"decimal","list-style-position":"outside"}},[(0,l._)("a",{href:"http://cg.iit.bme.hu/~szirmay/paper50_electronic.pdf"}," KELEMEN, C., SZIRMAY-KALOS, L., ANTAL, G., AND CSONKA, F. 2002. A simple and robust mutation strategy for the Metropolis light transport algorithm. Computer Graphics Forum 21, 3, 531–540. ")]),(0,l._)("li",{id:"cite-nice",style:{"text-align":"left","list-style-type":"decimal","list-style-position":"outside"}},[(0,l._)("a",{href:"https://doi.org/10.48550/arXiv.1410.8516"}," Laurent Dinh, David Krueger and Yoshua Bengio. 2015. NICE: Non-linear Independent Components Estimation. https://doi.org/10.48550/arXiv.1410.8516 ")]),(0,l._)("li",{id:"cite-realNVP",style:{"text-align":"left","list-style-type":"decimal","list-style-position":"outside"}},[(0,l._)("a",{href:"https://doi.org/10.48550/arXiv.1605.08803"}," Laurent Dinh, Jascha Sohl-Dickstein and Samy Bengio. 2017. Density estimation using Real NVP. https://doi.org/10.48550/arXiv.1605.08803 ")]),(0,l._)("li",{id:"cite-realNVP",style:{"text-align":"left","list-style-type":"decimal","list-style-position":"outside"}},[(0,l._)("a",{href:"https://doi.org/10.48550/arXiv.1807.03039"}," Diederik P. Kingma and Prafulla Dhariwal. 2018. Glow: Generative Flow with Invertible 1x1 Convolutions. https://doi.org/10.48550/arXiv.1807.03039 ")]),(0,l._)("li",{id:"cite-nis",style:{"text-align":"left","list-style-type":"decimal","list-style-position":"outside"}},[(0,l._)("a",{href:"https://dl.acm.org/doi/10.1145/3341156"}," Thomas Müller, Brian Mcwilliams, Fabrice Rousselle, Markus Gross, and Jan Novák. 2019. Neural Importance Sampling. ACM Trans. Graph. 38, 5, Article 145 (October 2019), 19 pages. https://doi.org/10.1145/3341156 ")]),(0,l._)("li",{id:"cite-npssis",style:{"text-align":"left","list-style-type":"decimal","list-style-position":"outside"}},[(0,l._)("a",{href:"https://arxiv.org/abs/1808.07840"}," Quan Zheng and Matthias Zwicker. 2018. Learning to Importance Sample in Primary Sample Space. https://arxiv.org/abs/1808.07840 ")]),(0,l._)("li",{id:"cite-ris",style:{"text-align":"left","list-style-type":"decimal","list-style-position":"outside"}},[(0,l._)("a",{href:"https://dl.acm.org/doi/10.5555/2383654.2383674"}," Justin F. Talbot, David Cline, and Parris Egbert. 2005. Importance resampling for global illumination. In Proceedings of the Sixteenth Eurographics conference on Rendering Techniques (EGSR '05). Eurographics Association, Goslar, DEU, 139–146. ")]),(0,l._)("li",{id:"cite-otflow",style:{"text-align":"left","list-style-type":"decimal","list-style-position":"outside"}},[(0,l._)("a",{href:"https://arxiv.org/abs/2006.00104#:~:text=OT%2DFlow%3A%20Fast%20and%20Accurate%20Continuous%20Normalizing%20Flows%20via%20Optimal%20Transport,-Derek%20Onken%2C%20Samy&text=A%20normalizing%20flow%20is%20an,density%20estimation%20and%20statistical%20inference."}," Derek Onken, Samy Wu Fung, Xingjian Li, and Lars Ruthotto. 2021. OT-Flow: Fast and Accurate Continuous Normalizing Flows via Optimal Transport. ")]),(0,l._)("li",{id:"cite-cpflow",style:{"text-align":"left","list-style-type":"decimal","list-style-position":"outside"}},[(0,l._)("a",{href:"chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Fopenreview.net%2Fpdf%3Fid%3Dte7PVH1sPxJ#=&zoom=auto"}," Chin-Wei Huang, Ricky T. Q. Chen, Christos Tsirigotis, and Aaron Courville. 2021. Convex Potential Flows: Universal Probability Distribution With Optimal Transport and Covnex Optimization. ")])],-1)));function _t(e,t,i,a,s,n){const o=(0,l.up)("el-header"),r=(0,l.up)("el-row"),_t=(0,l.up)("el-image"),kt=(0,l.up)("el-col"),xt=(0,l.up)("math-jax"),vt=(0,l.up)("el-main"),Wt=(0,l.up)("el-divider"),Tt=(0,l.up)("el-link"),Ut=(0,l.up)("el-footer"),It=(0,l.up)("el-container");return(0,l.wg)(),(0,l.iD)("div",u,[(0,l._)("div",m,[(0,l.Wm)(It,null,{default:(0,l.w5)((()=>[(0,l.Wm)(It,null,{default:(0,l.w5)((()=>[(0,l.Wm)(o,null,{default:(0,l.w5)((()=>[c])),_:1}),(0,l.Wm)(It,null,{default:(0,l.w5)((()=>[(0,l.Wm)(It,null,{default:(0,l.w5)((()=>[(0,l.Wm)(vt,null,{default:(0,l.w5)((()=>[(0,l.Wm)(r,{class:"row-bg"},{default:(0,l.w5)((()=>[p])),_:1}),(0,l.Wm)(r,{class:"row-bg"},{default:(0,l.w5)((()=>[h,d,f,g,w,y])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[b])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[_])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[k])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[x])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[v,W,T,(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,{span:17},{default:(0,l.w5)((()=>[(0,l.Wm)(_t,{style:{align:"center"},src:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/20230324094213.png"}),U])),_:1})])),_:1}),I,j])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,null,{default:(0,l.w5)((()=>[M,P])),_:1})])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[L])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,null,{default:(0,l.w5)((()=>[N,S])),_:1})])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[A,B])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,{span:17},{default:(0,l.w5)((()=>[C])),_:1})])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[D])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[Z])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[z])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[O])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[R,F,X,(0,l._)("p",V,[(0,l.Uk)(" Zheng et al. "),q,(0,l.Uk)(" did a simpler but inspiring work. They basically use the RealNVP to importance sample the primal space vector for path construction. It really provides a lot technical details for me to implement this stuff, and actually what I implemented in PyTorch is very similar to their work. A practical contribution is about how to take normalizing flow which is primarily on "),(0,l.Wm)(xt,{latex:"{\\mathbb{R}}^n"}),(0,l.Uk)(" into "),(0,l.Wm)(xt,{latex:"[0,1]^n"}),(0,l.Uk)(". ")])])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(_t,{style:{align:"center"},src:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/20230321230003.png"}),G])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[(0,l._)("p",E,[(0,l.Uk)(" They use a scaling and logit and their inverse in the start and their inverse (sigmoid and inverse scaling) in the end of the flow to mapping between "),(0,l.Wm)(xt,{latex:"[0,1]^n"}),(0,l.Uk)(" and "),(0,l.Wm)(xt,{latex:"{\\mathbb{R}}^n"}),(0,l.Uk)(". The scaling layer is actually a constant scaling tha maps "),(0,l.Wm)(xt,{latex:"[0,1]"}),(0,l.Uk)(" to "),(0,l.Wm)(xt,{latex:"[\\epsilon,1-\\epsilon]"}),(0,l.Uk)(" which prevents logit producing too large number, which will cause severe numerical problem and introducing more NaN in both results and gradients. ")]),H,K,(0,l._)("p",$,[(0,l.Uk)(" The KL divergence between target distribution "),(0,l.Wm)(xt,{latex:"p(x)"}),(0,l.Uk)(" and the learned distribution "),(0,l.Wm)(xt,{latex:"q(x)"}),(0,l.Uk)(" is: ")])])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l._)("p",null,[(0,l.Wm)(xt,{latex:"$$\n                  \\begin{aligned}\n                  KL(p||q;\\theta) &= \\int_\\Omega p(x) \\log \\frac{p(x)}{q(x;\\theta)} {\\rm d}x\\\\\n                                  &= \\int_\\Omega p(x) \\log p(x) {\\rm d}x - \\int_\\Omega p(x) \\log q(x;\\theta) {\\rm d}x\\\\\n                  \\end{aligned}\n                  $$"})])])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[(0,l._)("p",J,[(0,l.Uk)(" In practice we could not get "),(0,l.Wm)(xt,{latex:"p(x)"}),(0,l.Uk)(" as we do not know the normalizing term, and we could not differentiate the "),(0,l.Wm)(xt,{latex:"f(x)"}),(0,l.Uk)(" with respect to "),(0,l.Wm)(xt,{latex:"x"}),(0,l.Uk)(" ("),Y,(0,l.Uk)("). Anyway only the second term, the cross entropy term, has gradient. And the exciting part is: ")])])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l._)("p",null,[(0,l.Wm)(xt,{latex:"$$\n                  \\begin{aligned}\n                  \\nabla_\\theta KL(p||q;\\theta) &= -\\nabla_\\theta \\int_\\Omega p(x)\\log q(x;\\theta) {\\rm d}x\\\\\n                                  &= \\mathbb{E}_{q(x;\\theta)} \\left[-\\frac{p(X)}{q(X;\\theta)} \\nabla_\\theta \\log q(X;\\theta) \\right]\\\\\n                  \\end{aligned}\n                  $$"})])])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[(0,l._)("p",Q,[(0,l.Uk)(" where the expectation is over "),(0,l.Wm)(xt,{latex:"X\\sim q(x;\\theta) "}),(0,l.Uk)(", i.e. the samples are drawn from the learned generative model. Even if we could not know the normalizing term "),(0,l.Wm)(xt,{latex:"F"}),(0,l.Uk)(", we could still simply use "),(0,l.Wm)(xt,{latex:"f(x)=F\\cdot p(x)"}),(0,l.Uk)(" to substitute "),(0,l.Wm)(xt,{latex:"p(x)"}),(0,l.Uk)(", as it would only introduce a constant factor in the gradient, which is not harmful for gradient descent. It just shows that minimizing the KL divergence via gradient descent is equivalent to minimizing the negative log likelihood weighted by Monte Carlo estimates of F . ")])])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[ee])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[te,ie])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(_t,{style:{align:"center"},src:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/expressivity.png"}),ae])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[se,ne])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(_t,{style:{align:"center"},src:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/uniform%2Btrain.gif"}),le])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[oe,re])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(_t,{style:{align:"center"},src:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/image_train.gif"}),ue])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[me])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[ce])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[pe])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[he])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,{span:8},{default:(0,l.w5)((()=>[(0,l.Wm)(_t,{style:{align:"center"},src:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/20230325132820.png"}),de])),_:1})])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,{span:8},{default:(0,l.w5)((()=>[(0,l.Wm)(_t,{style:{align:"center"},src:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/20230325132813.png"}),fe])),_:1})])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[(0,l._)("p",ge,[(0,l.Uk)(" The original way of doing Metropolis-Hastings is tracking a Markov Chain on primal state x. But in Hidden Markov Model, we track a hidden state instead and generate X from another distribution "),(0,l.Wm)(xt,{latex:"p(x|z)"}),(0,l.Uk)(". Now let's substitute it with a known bijection defined by the neural network for neural importance sampling. ")])])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,{span:8},{default:(0,l.w5)((()=>[(0,l.Wm)(_t,{style:{align:"center"},src:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/20230325133905.png"}),we])),_:1})])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[(0,l._)("p",ye,[(0,l.Uk)(" Ideally the neural network should provides a bijection maps a uniform distribution Z to target distribution X, by some kind of warping. Thus we could do mutation like this:"),be,(0,l.Uk)(" (1) Start from the start state "),(0,l.Wm)(xt,{latex:"X"}),(0,l.Uk)(". "),_e,(0,l.Uk)(" (2) Mapping to latent space "),(0,l.Wm)(xt,{latex:"Z=g(X)"}),(0,l.Uk)(" . "),ke,(0,l.Uk)(" (3) Do common mutation in latent space "),(0,l.Wm)(xt,{latex:"Z'=mutate(Z)"}),(0,l.Uk)(". "),xe,(0,l.Uk)(" (4) Mapping to primal space "),(0,l.Wm)(xt,{latex:"X'=f(Z')"}),(0,l.Uk)(" . "),ve]),We,Te])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[Ue])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[(0,l._)("p",Ie,[(0,l.Uk)(" Let's start with something simpler, so consider the large step mutation first. In primal space, we uniform sample a random vector as proposal, which is very likely to be rejected. Using latent space mutation, we first generate a random vector in latent space and then map into primal space. Ideally, this mapping "),(0,l.Wm)(xt,{latex:"g(z)"}),(0,l.Uk)(" could produce x porportial to target distribution "),(0,l.Wm)(xt,{latex:"p"}),(0,l.Uk)(". ")]),(0,l._)("p",je,[(0,l.Uk)(" Considering detailed balance condition for "),(0,l.Wm)(xt,{latex:"X"}),(0,l.Uk)(" could be a little bit confusing, as describing "),(0,l.Wm)(xt,{latex:"T(X\\rightarrow X')"}),(0,l.Uk)(" is not clear. But we could consider the latent/hidden Markov chain, the underlying latent distribution "),(0,l.Wm)(xt,{latex:"Z"}),(0,l.Uk)(" is also equilibrium to a uniform distribution. "),(0,l.Wm)(xt,{latex:"f(Z)=f(Z')=1"}),(0,l.Uk)(" and "),(0,l.Wm)(xt,{latex:"T(Z\\rightarrow Z')=T(Z'\\rightarrow Z)=1"}),(0,l.Uk)(", thus the classical acceptance ratio is: "),(0,l.Wm)(xt,{latex:"a(Z\\rightarrow Z')=1"}),(0,l.Uk)(". ")]),(0,l._)("p",Me,[(0,l.Uk)(" Thus what actually happen is we proporse a random vector proportional to "),(0,l.Wm)(xt,{latex:"p"}),(0,l.Uk)(", and then we always accept it. This is exactly the process of optimal importance sampling, which is the best way to do sampling. It does immediate convergence and has no correlation between samples. ")])])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[Pe])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[Le])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,{span:14},{default:(0,l.w5)((()=>[(0,l.Wm)(_t,{style:{align:"center"},src:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/20230325135516.png"}),Ne])),_:1})])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[Se])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,{span:14},{default:(0,l.w5)((()=>[(0,l.Wm)(_t,{style:{align:"center"},src:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/20230325135315.png"}),Ae])),_:1})])),_:1}),(0,l._)("p",Be,[(0,l.Uk)(" Again when the underlying latent distribution "),(0,l.Wm)(xt,{latex:"Z"}),(0,l.Uk)(" is equilibrium to a uniform distribution, "),(0,l.Wm)(xt,{latex:"f(Z)=f(Z')=1"}),(0,l.Uk)(" and "),(0,l.Wm)(xt,{latex:"T(Z\\rightarrow Z')=T(Z'\\rightarrow Z)=1"}),(0,l.Uk)(", and the classical acceptance ratio is again: "),(0,l.Wm)(xt,{latex:"a(Z\\rightarrow Z')=1"}),(0,l.Uk)(". Which means we has 1 acceptance rate and 0 rejection rate, while still be able to exporling the neighbor space. ")]),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[Ce])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[De,Ze,(0,l._)("p",ze,[(0,l.Uk)(" Something different is we could no longer do acceptance in latent space. As the mapping is not perfect, a uniform sampling in Z would not recover the correct p(x) in primal space, so we should do acceptancein primal space. But this would introduce a new problem: we must evaluate the transition probability "),(0,l.Wm)(xt,{latex:"T(X\\rightarrow X')"}),(0,l.Uk)(". I am not sure in this part, it might be similar to primal mutation or need some additional Jacobian, I need to do more experiments on this. ")])])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[Oe])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[Re,Fe])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,{span:10},{default:(0,l.w5)((()=>[(0,l.Wm)(_t,{style:{align:"center"},src:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/20230325213419.png"}),Xe])),_:1})])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[Ve])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,{span:12},{default:(0,l.w5)((()=>[(0,l.Wm)(_t,{style:{align:"center"},src:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/20230325213359.png"})])),_:1})])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[qe])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[Ge])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[Ee,He,Ke,$e])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,null,{default:(0,l.w5)((()=>[Je,Ye])),_:1})])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[Qe])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,{span:11},{default:(0,l.w5)((()=>[(0,l.Wm)(_t,{style:{align:"center"},src:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/learned.png"}),et])),_:1})])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[tt])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,{span:12},{default:(0,l.w5)((()=>[(0,l.Wm)(_t,{style:{align:"center"},src:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/20230325160712.png"}),it])),_:1})])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[at])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,null,{default:(0,l.w5)((()=>[st,nt])),_:1})])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[lt])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,null,{default:(0,l.w5)((()=>[ot,rt])),_:1})])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[ut])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,{span:12},{default:(0,l.w5)((()=>[(0,l.Wm)(_t,{style:{align:"center"},src:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/20230325184734.png"}),mt])),_:1})])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[ct])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"center"},{default:(0,l.w5)((()=>[(0,l.Wm)(kt,{span:9},{default:(0,l.w5)((()=>[(0,l.Wm)(_t,{style:{align:"center"},src:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/20230325184745.png"}),pt])),_:1})])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[ht])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[dt])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[ft])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[gt,wt])),_:1}),(0,l.Wm)(r,{class:"row-bg",justify:"left"},{default:(0,l.w5)((()=>[yt,bt])),_:1})])),_:1}),(0,l.Wm)(Wt),(0,l.Wm)(Ut,null,{default:(0,l.w5)((()=>[(0,l.Wm)(Tt,{href:"https://suikasibyl.github.io/",target:"_blank",type:"primary"},{default:(0,l.w5)((()=>[(0,l.Uk)("My Homepage")])),_:1})])),_:1})])),_:1})])),_:1})])),_:1})])),_:1})])])}var kt=i(194),xt=i.n(kt),vt={name:"ReportPage",mounted(){new(xt())({el:"#udptvsbdpt",beforeImg:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/udpt500small.png",afterImg:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/bdpt500small.png"}),new(xt())({el:"#bdptvsmlt",beforeImg:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/bdpt500small.png",afterImg:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/mlt500small_fix.png"}),new(xt())({el:"#alldepthvsdpeth4",beforeImg:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/depth0to3.png",afterImg:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/depth3only.png"}),new(xt())({el:"#largestep",beforeImg:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/nonneural_50frame.png",afterImg:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/neural_50frame.png"}),new(xt())({el:"#smallstep",beforeImg:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/nonneural_200small.png",afterImg:"https://imagehost-suikasibyl-us.oss-us-west-1.aliyuncs.com/img/neural_200small.png"})},props:{},data(){return{formula:"$$x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}.$$"}}},Wt=i(744);const Tt=(0,Wt.Z)(vt,[["render",_t],["__scopeId","data-v-6137ba46"]]);var Ut=Tt,It={name:"App",components:{ReportPage:Ut}};const jt=(0,Wt.Z)(It,[["render",o]]);var Mt=jt;function Pt(){const e=document.getElementById("elementId");(0,n.kr)(e)}(0,n.c1)({},Pt);const Lt=(0,a.ri)(Mt);Lt.use(s.Z),Lt.use(n.ZP),Lt.mount("#app")}},t={};function i(a){var s=t[a];if(void 0!==s)return s.exports;var n=t[a]={exports:{}};return e[a].call(n.exports,n,n.exports,i),n.exports}i.m=e,function(){var e=[];i.O=function(t,a,s,n){if(!a){var l=1/0;for(m=0;m<e.length;m++){a=e[m][0],s=e[m][1],n=e[m][2];for(var o=!0,r=0;r<a.length;r++)(!1&n||l>=n)&&Object.keys(i.O).every((function(e){return i.O[e](a[r])}))?a.splice(r--,1):(o=!1,n<l&&(l=n));if(o){e.splice(m--,1);var u=s();void 0!==u&&(t=u)}}return t}n=n||0;for(var m=e.length;m>0&&e[m-1][2]>n;m--)e[m]=e[m-1];e[m]=[a,s,n]}}(),function(){i.n=function(e){var t=e&&e.__esModule?function(){return e["default"]}:function(){return e};return i.d(t,{a:t}),t}}(),function(){i.d=function(e,t){for(var a in t)i.o(t,a)&&!i.o(e,a)&&Object.defineProperty(e,a,{enumerable:!0,get:t[a]})}}(),function(){i.g=function(){if("object"===typeof globalThis)return globalThis;try{return this||new Function("return this")()}catch(e){if("object"===typeof window)return window}}()}(),function(){i.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)}}(),function(){var e={143:0};i.O.j=function(t){return 0===e[t]};var t=function(t,a){var s,n,l=a[0],o=a[1],r=a[2],u=0;if(l.some((function(t){return 0!==e[t]}))){for(s in o)i.o(o,s)&&(i.m[s]=o[s]);if(r)var m=r(i)}for(t&&t(a);u<l.length;u++)n=l[u],i.o(e,n)&&e[n]&&e[n][0](),e[n]=0;return i.O(m)},a=self["webpackChunkmltwebpage"]=self["webpackChunkmltwebpage"]||[];a.forEach(t.bind(null,0)),a.push=t.bind(null,a.push.bind(a))}();var a=i.O(void 0,[998],(function(){return i(933)}));a=i.O(a)})();
//# sourceMappingURL=app.5daf84ba.js.map